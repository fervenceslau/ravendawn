{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CaveMapper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9780tF7m2/fgg/oalVs0F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fervenceslau/ravendawn/blob/main/CaveMapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcQ2ShO4w2Sp"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import sys\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def CropImage(image, scale):\n",
        "    \"\"\"\n",
        "    Crops a region inside a 3-channel image relative to its dimensions through a scaling factor.\n",
        "    \"\"\"\n",
        "    scaleX, scaleY = [scale, scale]\n",
        "    if (type(scale) == list):\n",
        "        scaleX, scaleY = scale\n",
        "    width  = image.shape[1]\n",
        "    height = image.shape[0]\n",
        "    left   = int(0.5 * (1 - scaleX) * width)\n",
        "    right  = int(left + width * scaleX)\n",
        "    top    = int(0.5 * (1 - scaleY) * height)\n",
        "    bottom = int(top + height * scaleY)\n",
        "    return image[top:bottom, left:right, :]\n",
        "\n",
        "def RemovePlayer(image):\n",
        "    \"\"\"\n",
        "    Fills a black rectangle inside a 3-channel image position where the player should be.\n",
        "    \"\"\"\n",
        "    return cv.rectangle(image, (850, 426), (1006, 558), (0, 0 ,0), -1)\n",
        "\n",
        "def ResizeImage(image, scale):\n",
        "    \"\"\"\n",
        "    Resizes an image with a global scaling factor.\n",
        "    \"\"\"\n",
        "    width  = int(image.shape[1] * scale)\n",
        "    height = int(image.shape[0] * scale)\n",
        "    dim = (width, height)\n",
        "    return cv.resize(image, dim)\n",
        "\n",
        "def SelectRegionForFeatures(image, scale):\n",
        "    \"\"\"\n",
        "    Selects a region inside a 3-channel image to find tracking features.\n",
        "    The region is a cropped rectangle around the player without the player.\n",
        "    \"\"\"\n",
        "    image = RemovePlayer(image)\n",
        "    image = CropImage(image, scale)\n",
        "    return image\n",
        "\n",
        "def DetectAndDescribe(image):\n",
        "    \"\"\"\n",
        "    Detects and returns keypoints and features used to track movement between images.\n",
        "    \"\"\"\n",
        "    descriptor = cv.BRISK_create(thresh=10)\n",
        "    (kps, features) = descriptor.detectAndCompute(image, None)\n",
        "    return (kps, features)\n",
        "\n",
        "def MatchKeyPointsBF(featuresA, featuresB):\n",
        "    \"\"\"\n",
        "    Returns matched features using a brute-force descriptor matcher.\n",
        "    \"\"\"\n",
        "    bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)\n",
        "    best_matches = bf.match(featuresA, featuresB)\n",
        "    rawMatches = sorted(best_matches, key = lambda x:x.distance)\n",
        "    return rawMatches\n",
        "\n",
        "def EstimateMovement(kpsA, kpsB, matches):\n",
        "    \"\"\"\n",
        "    Estimates movement based on matching keypoints and estimateAffinePartial2D.\n",
        "    The movement is the last column of the affine matrix [R p], but since the estimation process\n",
        "    can be faulty, first we remove the influence of the rotation matrix [R], to get [I R^{-1}p].\n",
        "    \"\"\"\n",
        "    kpsA = np.float32([kp.pt for kp in kpsA])\n",
        "    kpsB = np.float32([kp.pt for kp in kpsB])\n",
        "    if len(matches) > 4:\n",
        "        ptsA = np.float32([kpsA[m.queryIdx] for m in matches])\n",
        "        ptsB = np.float32([kpsB[m.trainIdx] for m in matches])\n",
        "        (affineMatrix, inliers) = cv.estimateAffinePartial2D (ptsA, ptsB)\n",
        "        movement = np.matmul(np.linalg.inv(affineMatrix[:, 0:2]), affineMatrix[:, 2])\n",
        "        return movement\n",
        "    else:\n",
        "        return None\n",
        "    \n",
        "def CalculateImageLimits(baseShape, newShape, movement):\n",
        "    \"\"\"\n",
        "    Calculates the output image limits after applying a movement to a new image.\n",
        "    With these limits, calculate the image dimensions and offset to make the minimum coordinates (0, 0) once again.\n",
        "    \"\"\"\n",
        "    baseLimits = np.array([[0, 0], np.flip(baseShape[0:-1]) - [1, 1]]).T\n",
        "    newLimits  = np.array([[0, 0], np.flip(newShape[0:-1]) - [1, 1]]).T + np.tile(movement, (2, 1)).T\n",
        "    limits     = np.concatenate((baseLimits, newLimits), axis=1)\n",
        "    limits     = np.vstack((limits.min(axis=1), limits.max(axis=1))).T\n",
        "    offset     = -limits[:, 0]\n",
        "    dimension  = (limits[:, 1] + offset + [1, 1]).astype(np.int32)\n",
        "    return limits, offset, dimension\n",
        "\n",
        "def OffsetImage(image, offset, dimension):\n",
        "    \"\"\"\n",
        "    Applies an offset to a given image and make the resulting image have the desired dimension.\n",
        "    \"\"\"\n",
        "    offset = offset.astype(np.int32)\n",
        "    result = np.zeros(np.hstack((np.flip(dimension), 3))).astype(np.uint8)\n",
        "    result[offset[1]:(offset[1] + image.shape[0]),\n",
        "           offset[0]:(offset[0] + image.shape[1]),\n",
        "           :] = image\n",
        "    return result\n",
        "\n",
        "def StitchImages(imageA, imageB, movement, offset, dimension):\n",
        "    \"\"\"\n",
        "    Stitch two images together based on the relative movement. \n",
        "    The images are blended together using max() function to keep high brightness values.\n",
        "    The output image will have the desired dimensions and the offset variable is used to correct the minimum\n",
        "    limit of the resulting image to make it (0, 0).\n",
        "    \"\"\"\n",
        "    auxA = OffsetImage(imageA, offset, dimension)\n",
        "    auxB = OffsetImage(imageB, offset + movement, dimension)\n",
        "    result = np.maximum(auxA, auxB)\n",
        "    return result.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/fervenceslau/ravendawn.git\n",
        "\n",
        "# Create a VideoCapture object and check if file was opened successfully\n",
        "cap = cv.VideoCapture('sample.mp4')\n",
        "if (cap.isOpened()== False): \n",
        "  print(\"Error opening video file\")\n",
        "  sys.exit()\n",
        "\n",
        "# Get information from video file\n",
        "height     = cap.get(cv.CAP_PROP_FRAME_HEIGHT)\n",
        "width      = cap.get(cv.CAP_PROP_FRAME_WIDTH) \n",
        "fps        = cap.get(cv.CAP_PROP_FPS)\n",
        "frameCount = cap.get(cv.CAP_PROP_FRAME_COUNT)\n",
        "duration   = frameCount / fps\n",
        "\n",
        "# Define video times to use in the algorithm\n",
        "videoTimeStart = 5\n",
        "videoTimeStop  = 0    # TODO: add stop time functionality\n",
        "videoTimeStep  = 0.1\n",
        "\n",
        "# Frame information used to read frames from the VideoCapture\n",
        "frameIdx    = 0\n",
        "frameStep   = videoTimeStep * fps\n",
        "frameStart  = videoTimeStart * fps\n",
        "frameNumber = int(frameStart + frameIdx * frameStep)\n",
        "frameTotal  = int((frameCount - frameNumber) / frameStep)\n",
        "\n",
        "# Define variables used in the stitching algorithm\n",
        "images = [[], []]\n",
        "result = []\n",
        "offset = np.array([0, 0])\n",
        "movement = np.array([0, 0])\n",
        "\n",
        "# Read frames from the VideoCapture and perform image stitching (using a progress bar)\n",
        "for frameIdx in tqdm(range(frameTotal)):\n",
        "    \n",
        "    # Read a frame\n",
        "    ret = False\n",
        "    while (not ret):\n",
        "        cap.set(1, frameNumber);   \n",
        "        ret, frame = cap.read()\n",
        "        frame = frame[1:-64, 1:-1, :] # Removes black outline from screenshot\n",
        "\n",
        "    # Update images list\n",
        "    images[0] = images[1]\n",
        "    images[1] = frame\n",
        "\n",
        "    # Update sample number\n",
        "    frameNumber = int(frameStart + frameIdx * frameStep)\n",
        "\n",
        "    # Stich images after acquiring two images\n",
        "    if (frameIdx > 1):\n",
        "\n",
        "        # Select image regions used to track movement\n",
        "        imgA = SelectRegionForFeatures(images[0], [0.5, 0.5])\n",
        "        imgB = SelectRegionForFeatures(images[1], [0.5, 0.5])\n",
        "\n",
        "        # Obtain tracking keypoints and features\n",
        "        kpsA, featuresA = DetectAndDescribe(imgA)\n",
        "        kpsB, featuresB = DetectAndDescribe(imgB)\n",
        "\n",
        "        # Find matches between tracking points\n",
        "        matches = MatchKeyPointsBF(featuresA, featuresB)\n",
        "\n",
        "        # Initializes resulting stitched image image\n",
        "        if (len(result) == 0):\n",
        "            result = images[0]\n",
        "\n",
        "        # Calculate overall new frame movement and compensate for the previous offset\n",
        "        # The estimated movement is rounded to avoid error accumulation caused by subpixel movements...\n",
        "        movement = movement + -np.round(EstimateMovement(kpsA, kpsB, matches))\n",
        "        movement = movement + offset\n",
        "\n",
        "        # Calculate the new image limits, offset and dimension\n",
        "        limits, offset, dimension = CalculateImageLimits(result.shape, images[1].shape, movement)\n",
        "\n",
        "        # Stitch new frame to the results\n",
        "        result = StitchImages(result, images[1], movement, offset, dimension)\n",
        "\n",
        "        # Write the result to an output file every N frames\n",
        "        if (frameIdx % 50 == 0):\n",
        "            cv.imwrite('result.png', result)\n",
        "\n",
        "        # Display the results for each step\n",
        "#         cv.imshow(\"\", result)\n",
        "#         if cv.waitKey() & 0xFF == ord('q'):\n",
        "#             break\n",
        "\n",
        "# Write the result to an output file after all code execution\n",
        "cv.imwrite('result.png', result)\n",
        "    \n",
        "# Releases VideoCapture and close all cv windows\n",
        "cap.release()\n",
        "cv.destroyAllWindows()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "sZCNGBo7zYdu",
        "outputId": "afb1f09f-5e7e-4519-c9c8-df68531aeb79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error opening video file\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SB-0ha221B40",
        "outputId": "916619dc-f436-49f9-e281-e7eaa8644cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    }
  ]
}